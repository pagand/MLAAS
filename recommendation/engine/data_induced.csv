id,title,number,overview,keywords,type,vote_average,votes_count,duration,category,meta
1,machine learning,1.1,"Machine learning is a subset of AI that enables computers to learn and improve from data without explicit programming. It uses algorithms to analyze data, identify patterns, and make predictions. This iterative process empowers machines to adapt and improve their performance over time.","Machine Learning, Artificial Intelligence, Data Analysis, Pattern Recognition, Predictions",concept,0.93,41,1,Introduction,
2,ML python packages,1.2,"Machine Learning is the science of programming computers to learn from data. It eliminates the need for explicit programming by utilizing algorithms and mathematical formulas. Python is a popular language for this task, with libraries like NumPy, SciPy, scikit-learn, TensorFlow, Keras, and PyTorch providing efficient solutions for data processing and analysis. These libraries enable tasks such as matrix processing, optimization, image manipulation, classical ML algorithms, deep learning, and neural network design, making Machine Learning more accessible and efficient than ever before.","Numpy, Scipy, Scikit-learn, TensorFlow, Keras, PyTorch, Pandas, Matplotlib, python, library",code,0.14,37,5,Introduction,
3,Type of ML,1.3,"Machine learning encompasses various algorithms, including supervised learning (using labeled data for prediction) and unsupervised learning (finding patterns in unlabeled data). These techniques enable tasks like classification, regression, clustering, and anomaly detection.","Machine learning, Supervised learning, Unsupervised learning, Classification, Clustering, Type",concept,0.41,42,1,Introduction,
4,Import library,1.4,"To utilize popular machine learning packages in Python, import scikit-learn (sklearn), Matplotlib, pandas, NumPy, and Seaborn. These libraries provide essential tools for data analysis, visualization, and machine learning algorithms.","Machine learning packages, scikit-learn, Matplotlib, pandas, NumPy",code,0.57,7,3,Introduction,
5,introduction to data,2.1,"Data is the foundation of Data Analytics, Machine Learning, and Artificial Intelligence. It encompasses unprocessed facts or information that drives research and automation. In Machine Learning, data is split into three categories: training data, used to train the model; validation data, for evaluating and improving the model during training; and testing data, which provides an unbiased evaluation of the trained model's performance.","training, test, validation, val, split, data",concept,0.49,53,2,data,
6,data preprocessing,2.2,"Data preprocessing is a crucial step in machine learning, where raw data is transformed to a clean and suitable format. This article explores three preprocessing techniques: rescaling data to a common scale, binarizing data using a threshold, and standardizing data to a Gaussian distribution. The code provided demonstrates how to implement these techniques using scikit-learn's MinMaxScaler, Binarizer, and StandardScaler classes.","rescale, normalize, whitening, noise, binary, Data preprocessing, Rescaling, Binarization, Standardization",code,0.18,75,7,data,
7,Cleaning data,2.3,"Data cleaning is a crucial step in machine learning to ensure reliable and accurate results. It involves removing duplicate and irrelevant observations, fixing structural errors, managing outliers, and handling missing data. Proper data cleaning improves the efficiency and quality of models. Various tools like Openrefine and Trifacta Wrangler facilitate the data cleaning process.","Removal, Unwanted, Fixing, Structural, Unwanted outliers, Data cleaning, Duplicate observations, Structural errors, Outliers, Missing data, Data cleansing tools.",concept,0.34,55,2,data,
8,Feature scaling,2.4,"Feature scaling is a crucial step in data preprocessing to ensure fair treatment of independent features in machine learning. It standardizes the values to a fixed range, preventing algorithms from favoring certain features due to varying magnitudes or units. Min-Max Normalization and Standardization are popular techniques used for feature scaling.","min-max, range, magnitude, normalization, Standardization,  Feature scaling, Data preprocessing, Normalization",code,0.36,21,5,data,
9,Handle missing data,2.5,"Imbalanced data distribution is a common challenge in machine learning, where the class distribution is significantly skewed. Standard algorithms often favor the majority class, leading to poor performance on the minority class. Two popular techniques for handling imbalanced data are SMOTE (Synthetic Minority Oversampling Technique) for oversampling and NearMiss Algorithm for undersampling. This article provides insights and practical examples on how to choose the best technique for addressing imbalanced data.","Imbalanced, Data Distribution, minority, majority, negligible, SMOTE, Undersampling, NearMiss, unbalanced, up sampling, Oversampling.",code,0.57,75,12,data,
10,train-test split,2.6,The train-test split is a crucial step in machine learning to evaluate model performance. This technique divides the dataset into training and testing sets. Python's scikit-learn library provides a convenient train_test_split function for this purpose.,"train-test split, machine learning, evaluation, dataset, training set, test set, scikit-learn, Python.",code,0.35,64,2,data,
11,categorical data,2.7,"In feature engineering, converting categorical variables to numerical values is crucial for ML. One-hot encoding creates binary columns for each category, while label encoding assigns numerical labels. The choice depends on data and algorithms. Code examples demonstrate one-hot encoding using pandas and label encoding using scikit-learn.","Categorical variables, numerical values, feature engineering, one-hot encoding, label encoding, pandas, scikit-learn.",code,0.91,33,4,data,
12,imbalance data,2.8,"Addressing imbalanced data is crucial in machine learning. Undersampling and oversampling techniques help to mitigate the issue. SMOTE is a popular oversampling method that generates synthetic samples. The provided code demonstrates the usage of SMOTE and RandomUnderSampler from the imbalanced-learn library to perform oversampling and undersampling, respectively.","imbalanced data, undersampling, oversampling, SMOTE, imbalanced-learn, RandomUnderSampler, synthetic samples, machine learning.",code,0.41,47,3,data,
13,Introduction to supervised learning,3.1,"Machine learning involves improving a machine's performance in a task based on its past experiences. In the context of predicting customer behavior, a machine learns from previous data to predict whether a customer will purchase a specific product. The training process involves splitting the data into training and testing sets, where the model learns from the training data and is evaluated on the unseen testing data. Supervised learning includes classification, where discrete labels are predicted, and regression, where continuous values are predicted. The accuracy of the model is assessed based on its ability to predict the correct labels or values.","Classification, Regression, label, supervised, past experiences, training the system, testing, supervised learning.",concept,0.42,14,2,Supervised learning,
14,Classification,3.2,"Classification is the task of categorizing data into sub-categories, performed by machines in machine learning and statistics. It involves identifying the category to which a new observation belongs based on a training set. There are two types of classification: binary classification and multiclass classification. Various classifiers, such as logistic regression, decision tree classifiers, and support vector machines, are used for classification tasks. These techniques find applications in self-driving cars, spam email filtering, health problem detection, facial recognition, and more. Classifiers can be categorized as discriminative or generative, depending on their modeling approach.","Binary, Linear Classifiers, multi class, Decision Tree, Support Vector Machines, Artificial Neural Networks, Bayesian Regression, Gaussian Naive, Bayes Classifiers, Stochastic Gradient Descent, SGD, Ensemble Methods, Random Forests, AdaBoost, Bagging Classifier, Voting Classifier, ExtraTrees Classifier, classification, multiclass classification, tree-based classifiers, practical applications, discriminative classifiers, generative classifiers.",concept,0.77,17,7,Supervised learning,
15,Classification using sklearn,3.3,"Multiclass classification involves training a classifier using a dataset consisting of training examples with features and corresponding labels. The dataset is split into training and test data, and classifiers like decision trees, SVM, and KNN are trained on the training data. These classifiers are then used to predict labels for the test data. Accuracy is measured, and classification results are visualized. The decision tree classifier poses questions to the dataset, creating a binary tree structure to classify the data.","Load dataset, Split dataset, Train model, predict for test, Measure accuracy, visualize classification,  multiclass classification, dataset, training examples, features, label, classifier, decision tree, SVM, KNN, accuracy, visualization.",code,0.73,27,5,Supervised learning,
16,cross validation,3.4,"Cross-validation methods such as k-fold and stratified k-fold are essential for evaluating machine learning models. K-fold divides the data into k subsets, training and testing the model on each fold. Stratified k-fold ensures class distribution balance. These methods help assess model performance on unseen data.","Cross-validation, k-fold cross-validation, stratified k-fold cross-validation, machine learning, model evaluation, model performance, code example, training data, testing data, accuracy.",code,0.37,89,5,Supervised learning,
17,evaluation metrics,3.5,"When evaluating a classification model in machine learning, metrics such as accuracy, precision, recall, and F1 score play a crucial role. These measurements assess the correctness, completeness, and overall performance of the model. Additionally, the confusion matrix provides a detailed breakdown of predictions. A solid grasp of these concepts is vital for effectively evaluating and improving classification models.","classification measurements, precision, recall, accuracy, F1 score, confusion matrix, machine learning, model evaluation.",code,0.1,89,3,Supervised learning,
18,AUC-ROC ,3.6,"AUC-ROC plot is a valuable tool for evaluating binary classifiers in ML. It visualizes the trade-off between true positive rate and false positive rate. The higher the AUC-ROC score, the better the classifier performance. The provided code demonstrates how to generate and plot the AUC-ROC curve using Python's scikit-learn library.","AUC-ROC, classification, evaluation metric, binary classifier, receiver operating characteristic, true positive rate, false positive rate, ROC curve, machine learning, scikit-learn.",code,0.36,42,4,Supervised learning,
19,Optimization,4.1,"Optimization techniques are fundamental in machine learning for finding optimal model parameters. Gradient Descent, Stochastic Gradient Descent, Adam, and RMSprop are commonly used algorithms. This description introduces these optimization methods, which are crucial for training accurate machine learning models.","optimization techniques, machine learning, Gradient Descent, Stochastic Gradient Descent, Adam, RMSprop, model parameters, loss function.",concept,0.34,81,5,optimization,
20,Gradient descent,4.2,"Gradient Descent is an optimization algorithm used for minimizing the cost function in machine learning. This article explains the different types of Gradient Descent, including Batch, Stochastic, and Mini-Batch, along with their convergence trends. The impact of the learning rate and the number of training examples on convergence is also discussed.","Batch Gradient Descent, Stochastic Gradient Descent, optimization, Mini Batch gradient descent, global minimum, Gradient Descent, cost function, Mini-Batch Gradient Descent, convergence, learning rate, training examples",concept,0.03,86,3,optimization,
21,Stochastic Gradient ,4.3,"Stochastic Gradient Descent (SGD) is an optimization algorithm that randomly selects a few samples from a dataset for each iteration, making it computationally efficient for large datasets. This article explains the concept of SGD, its advantages over Batch Gradient Descent, and the trade-off between convergence speed and noise in the descent process.","shuffled, iteration, noisier, minima, optimizing, stochastic, gradient, Stochastic Gradient Descent, optimization algorithm, batch size, dataset, computational efficiency, convergence, noise, back-propagation",code,0.48,96,3,optimization,
22,hyper parameter tuning,4.4,Hyperparameter tuning is the process of finding the optimal values for the hyperparameters of a machine learning model. Grid search is a common approach where a predefined set of hyperparameter values is searched exhaustively. The code example demonstrates using GridSearchCV to find the best parameters for an SVM model.,"hyperparameter tuning, machine learning, grid search, hyperparameters, model performance, optimal values.",code,0.16,47,4,optimization,
23,learning rate,4.5,"Optimization techniques play a vital role in machine learning by fine-tuning the model's performance. Learning rate, weight decay, warm-up, learning rate scheduling, and other methods influence the convergence and generalization of models. Choosing the appropriate techniques is crucial for achieving efficient and effective optimization.","learning rate, weight decay, warm-up, learning rate scheduling, optimization techniques, regularization, overfitting, momentum, adaptive methods.",concept,0.99,57,6,optimization,
24,Logistic Regression,5.1,"Logistic regression is a powerful algorithm for binary classification tasks. It models the relationship between input features and the probability of the positive class using the logistic function. By estimating the optimal coefficients through maximum likelihood estimation, logistic regression provides a reliable way to predict the probability of class membership.","classification, binary, probability, statistical, learning, algorithm, logistic regression, binary classification, sigmoid function, maximum likelihood estimation, scikit-learn.",concept,0.54,54,8,Algorithm,
25,Decision tree,5.2,Decision trees are classification algorithms that create a tree-like model based on feature splits. They provide interpretable results and can handle both categorical and numerical features. The Python snippet demonstrates training a decision tree classifier using scikit-learn on the Iris dataset.,"classification, probability, learning, algorithm, tree, decision, decision tree, classification, interpretable, scikit-learn, feature splits, top-down approach.",code,0.21,93,8,Algorithm,
26,Support Vector Classifier,5.3,"SVC is a versatile classification algorithm that finds the optimal hyperplane to separate different classes by maximizing the margin. It handles both linearly separable and non-linearly separable data using kernel functions, providing flexibility and accurate classification results.","support vector classifier, SVC, hyperplane, margin, support vectors, kernel functions, scikit-learn.",code,0.28,59,5,Algorithm,
27,K-Nearest Neighbors,5.4,"KNN is a simple and intuitive classification algorithm that classifies new samples based on the classes of its nearest neighbors. It is easy to understand and implement, making it a popular choice for classification tasks.","K-Nearest Neighbors, KNN, classification algorithm, nearest neighbors, hyperparameter, distance metric, scikit-learn.",code,0.55,68,6,Algorithm,
28,Random Forest,5.5,"Random Forest is a powerful ensemble learning algorithm that combines multiple decision trees to improve classification accuracy. It is robust, versatile, and effective in handling various types of datasets.","Random Forest, ensemble learning, decision trees, overfitting, feature importance, scikit-learn.",code,0.33,11,5,Algorithm,
29,Linear Discriminant Analysis,5.6,Linear Discriminant Analysis is a dimensionality reduction technique and classification algorithm that finds a linear projection maximizing class separability. It is commonly used for feature extraction and improving classification performance.,"Linear Discriminant Analysis, LDA, dimensionality reduction, supervised classification, Gaussian distribution, scikit-learn.",code,0.7,40,4,Algorithm,
30,Naive Bayes,5.7,Gaussian Naive Bayes is a probabilistic classification algorithm that assumes feature independence and Gaussian distribution. It is suitable for both binary and multiclass classification tasks and performs well on datasets with continuous features.,"Gaussian Naive Bayes, NB, classification, Bayes' theorem, probabilistic, scikit-learn.",code,0.76,87,7,Algorithm,
31,Classification and Regression Trees ,5.8,"CART is a powerful and interpretable algorithm that can handle both categorical and numerical features. It can capture complex relationships and interactions between features and is robust to outliers. However, CART is prone to overfitting if the tree is allowed to grow too deep, so regularization techniques like pruning or limiting the tree depth are often applied.","CART, decision tree, classification, regression, impurity measure, Gini impurity, entropy, top-down, information gain, scikit-learn.",code,0.2,27,5,Algorithm,
